# Provider Configuration
# Controls which LLM and embedding providers are used

providers:
  llm:
    # Priority order - system will try in order until one works
    priority:
      - openai
      - groq
      - ollama
    
    ollama:
      model: "llama3.2"
      base_url: "http://localhost:11434"
      enabled: false
      
    groq:
      model: "llama-3.1-70b-versatile"
      api_key_env: "GROQ_API_KEY"
      enabled: false
      
    openai:
      model: "gpt-4o-mini"
      api_key_env: "OPENAI_API_KEY"
      enabled: true
  
  embedding:
    priority:
      - local
      - openai
    
    local:
      model: "all-MiniLM-L6-v2"
      enabled: false
      
    openai:
      model: "text-embedding-3-small"
      api_key_env: "OPENAI_API_KEY"
      enabled: true
